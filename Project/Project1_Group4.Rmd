---
title: "EDA for Project 1"
author: "Alanna Hazlett"
date: "2024-06-13"
output: html_document
---

```{r}
# | message: FALSE
library(tidyverse)
library(tidymodels)
library(reshape2) # for melt function needed for graph
library(patchwork)
library(probably)  # for threshold_perf
```

Load Data
```{r `show_col_types = FALSE`}
# | message: FALSE
# | warning: FALSE
training_data<-read_csv("HaitiPixels.csv")
training_data<-training_data %>% 
    mutate(Class = as.factor(Class))

set1<-read_table("orthovnir057_ROI_NON_Blue_Tarps.txt", skip=8, col_names=FALSE)
set_1<-set1[-c(1:7)]  
colnames(set_1)<-c("B1","B2","B3")

set2<-read_table("orthovnir078_ROI_Blue_Tarps.txt", skip=8, col_names=FALSE)
set_2<-set2[-c(1:7)]
colnames(set_2)<-c("B1","B2","B3")

set3<-read_table("orthovnir069_ROI_NOT_Blue_Tarps.txt", skip=8, col_names=FALSE)
set_3<-set3[-c(1:7)]
colnames(set_3)<-c("B1","B2","B3")

set4<-read_table("orthovnir069_ROI_Blue_Tarps.txt", skip=8, col_names=FALSE)
set_4<-set4[-c(1:7)]
colnames(set_4)<-c("B1","B2","B3")

set5<-read_table("orthovnir067_ROI_NOT_Blue_Tarps.txt", skip=8, col_names=FALSE)
set_5<-set5[-c(1:7)]
colnames(set_5)<-c("B1","B2","B3")

set6<-read_table("orthovnir067_ROI_Blue_Tarps.txt", skip=8, col_names=FALSE)
set_6<-set6[-c(1:7)]
colnames(set_6)<-c("B1","B2","B3")

set7<-read_table("orthovnir078_ROI_NON_Blue_Tarps.txt", skip=8, col_names=FALSE)
set_7<-set7[-c(1:7)]
colnames(set_7)<-c("B1","B2","B3")


#Same as set 6 
#set8<-read_table("orthovnir067_ROI_Blue_Tarps_data.txt")
#set_8<-set8[-c(4)]
#colnames(set_8)<-c("B1","B2","B3")

holdout <- rbind(set_1, set_2, set_3, set_4, set_5, set_6, set_7)
#colnames(holdout) <- c("X", "Y", "Map X", "Map Y", "Lat", "Lon", "B1", "B2", "B3")
```
```{r}
summary(training_data)
#There are no missing values
#training_data[!complete.cases(training_data),]

```

```{r}
#| fig.width: 15
#| fig.height: 5
#| dv: "png"
data_long<-training_data %>% 
  reshape2::melt()
g1<-ggplot(training_data,aes(x=Class))+
  geom_bar()+
  labs(title="Count of Class")
g2<-training_data %>% 
  group_by(Class) %>% 
  summarize(counts=n()) %>% 
  mutate(percent=counts/nrow(training_data)) %>% 
    ggplot(aes(x=Class,y=percent))+
      geom_bar(stat="identity")+
      labs(x="Class",y="Proportion",title="Proportion of Class of Images")  
g3<-ggplot(training_data,aes(x=Red))+
  geom_histogram(bins=15,fill="red")+
  labs(title="Distribution of Red")
g4<-ggplot(training_data,aes(x=Green))+
  geom_histogram(bins=15,fill="green")+
  labs(title="Distribution of Green")
g5<-ggplot(training_data,aes(x=Blue))+
  geom_histogram(bins=15,fill="blue")+
  labs(title="Distribution of Blue")
g6<-training_data %>% 
  melt() %>% 
  mutate(Color=as.factor(variable)) %>% 
  ggplot( aes(x=Class, y=value, fill=Color))+
  geom_boxplot()+
  labs(x="Class", y="Value", title="Training Data Distribution of Color Value by Class")

g1 + g2 

g6
```

```{r}
#| fig.width: 8
#| fig.height: 4
#Get rid of scientific notation
options(scipen=999)
gh1<-ggplot(holdout,aes(x=B1))+
  geom_histogram(bins=15)+
  labs(title="Distribution of B1")
gh2<-ggplot(holdout,aes(x=B2))+
  geom_histogram(bins=15)+
  labs(title="Distribution of B2")
gh3<-ggplot(holdout,aes(x=B3))+
  geom_histogram(bins=15)+
  labs(title="Distribution of B3")

g3 + gh1 #Maybe Red, large count near 250, most density around 50-125
g4 + gh2 #Maybe green, compared to B3 this does have significant count near 250. I propose green is more likely to retain higher pixel values than blue due to it's presence in nature. 
g5 + gh3 #Maybe blue majority of density is lower for B3 than for B1 or B2. 
```
\
* B1 appears similar to Red in distribution, there is a large count near 250 with most density around 50-125 pixels.
* B2 appears similar to Green in distribution. B2 compared to B3 has a more significant count near 250. We propose green is more likely to retain higher pixel values than blue due to it's presence in nature. B2's highest density is a higher pixel value that of B3, which aligns with Green and Blue respectively. 
* B3 appears similar to Blue in distribution. For Blue the majority of density is lower than Red or Green and this holds true for B3 compared to B1 or B2. 

```{r}
# try out the holdout set and find some proportions to identify the three colors
copy<-holdout
colnames(copy)<-c("B1", "B2", "B3")

## for the training data
training_data_copy<-training_data

prop_red<-training_data_copy[which((training_data_copy$Red > training_data_copy$Blue) & (training_data_copy$Red > training_data_copy$Green)),]

prop_green<- training_data_copy[which(( training_data_copy$Green >  training_data_copy$Blue) & ( training_data_copy$Green >  training_data_copy$Red)),]

prop_blue<- training_data_copy[which(( training_data_copy$Blue >  training_data_copy$Red) &  training_data_copy$Blue>  training_data_copy$Green),]

longlive_Red<-nrow(prop_red)[1]/nrow( training_data_copy)[1]

longlive_Green<-nrow(prop_green)[1]/nrow( training_data_copy)[1]


## testing data sets containing observations where the highlighted color is the 
## dominant one
prop_B1<-copy[which((copy$B1 > copy$B2) & (copy$B1 > copy$B3)),]
prop_B2<-copy[which((copy$B2 > copy$B1) & (copy$B2 > copy$B3)),]
prop_B3<-copy[which((copy$B3 > copy$B1) & (copy$B3 > copy$B2)),]


## we find the proportion of observations where B1 is dominating over the total number of observations
longlive_B1<-nrow(prop_B1)[1]/nrow(copy)[1]
longlive_B1

## ditto for B2
longlive_B2<-nrow(prop_B2)[1]/nrow(copy)[1]
longlive_B2

## remainder
1-(longlive_B1+longlive_B2)

## When compared to the original training set, we now confirm
## B1 is red, B2 equals green, B3 is blue
longlive_Red
longlive_Green
```


Adjust Holdout Set to assign Red, Green, Blue to B1, B2, and B3. 
```{r}
#New name = Old name
holdout<-rename(holdout, Red = B1)
holdout<-rename(holdout, Green = B2)
holdout<-rename(holdout, Blue = B3)
```


Make computing cluster
```{r}
library(doParallel)
cl <- makePSOCKcluster(parallel::detectCores(logical = FALSE))
registerDoParallel(cl)
```
Train the models
```{r}
binary_training_data<-training_data %>% 
  mutate(Class = as.factor(ifelse(Class=="Blue Tarp","BlueTarp","NotBlueTarp")))
formula <- Class ~ `Red` + `Green` + `Blue`
haiti_recipe <- recipe(formula, data=binary_training_data) %>%
    step_normalize(all_numeric_predictors())
logreg_spec <- logistic_reg(mode="classification") %>%
      set_engine('glm')
lda_spec <- discrim_linear(mode="classification") %>%
      set_engine('MASS')
qda_spec <- discrim_quad(mode="classification") %>%
      set_engine('MASS')
```

Combine preprocessing steps and model specification in workflow
```{r}
logreg_wf <- workflow() %>%
    add_recipe(haiti_recipe) %>%
    add_model(logreg_spec)
lda_wf <- workflow() %>%
    add_recipe(haiti_recipe) %>%
    add_model(lda_spec)
qda_wf <- workflow() %>%
    add_recipe(haiti_recipe) %>%
    add_model(qda_spec)
```

Cross-validation for model selection
- 10-fold cross-validation using stratified sampling
- Measure performance using ROC-AUC
- Save resample predictions, so that we can build ROC curves using cross-validation results
```{r}
resamples <- vfold_cv(binary_training_data, v=10, strata=Class)
model_metrics <- metric_set(roc_auc, accuracy,kap,j_index)

#When it performs resampling, default setting does not keep any info about results of each fold. Later when you want to do ROC Curve for cross validation results, you need the predictions. This specifies that you save the predictions. 
cv_control <- control_resamples(save_pred=TRUE)
```

Cross-validation
```{r cross-validation}
logreg_cv <- fit_resamples(logreg_wf, resamples, metrics=model_metrics, control=cv_control)
lda_cv <- fit_resamples(lda_wf, resamples, metrics=model_metrics, control=cv_control)
qda_cv <- fit_resamples(qda_wf, resamples, metrics=model_metrics, control=cv_control)
```
Metrics Table
```{r cv-metrics-table}
cv_metrics <- bind_rows(
        collect_metrics(logreg_cv) %>% mutate(model="Logistic regression"),
        collect_metrics(lda_cv) %>% mutate(model="LDA"),
        collect_metrics(qda_cv) %>% mutate(model="QDA")
    ) 
cv_metrics %>% 
    select(model, .metric, mean) %>%
    pivot_wider(names_from=".metric", values_from="mean") %>%
    knitr::kable(caption="Cross-validation performance metrics", digits=3)
```
\
Visualization of the same data
```{r cv-metrics-figure}
#| fig.cap: Cross-validation performance metrics
#| fig.width: 6
#| fig.height: 3
#| out.width: 75%
ggplot(cv_metrics, aes(x=mean, y=model, xmin=mean-std_err, xmax=mean+std_err)) +
    geom_point() + 
    geom_linerange() +
    facet_wrap(~ .metric)
```
\
Conclusions:\
* QDA: Seems to perform second best on the training data across all metrics.\
* Logistic: Appears to perform the best across all metrics.\
* LDA: Performed the worst across all metrics. 

* We know that accuracy is not going to be a good measure for this data, since it is an imbalanced dataset. We are better off using j index which has a better balance 

Overlayed ROC Curves
Overlay:
```{r cv-roc-curves-overlay}
#| fig.width: 6
#| fig.height: 4
#| fig.cap: Overlay of cross-validation ROC curves
roc_cv_data <- function(model_cv) {
    cv_predictions <- collect_predictions(model_cv)
    cv_predictions %>% 
        roc_curve(truth=Class, .pred_BlueTarp, event_level="first")
}
g1 = bind_rows(
    roc_cv_data(logreg_cv) %>% mutate(model="Logistic regression"),
    roc_cv_data(lda_cv) %>% mutate(model="LDA"),
    roc_cv_data(qda_cv) %>% mutate(model="QDA")
) %>%
ggplot(aes(x=1-specificity, y=sensitivity, color=model)) +
    geom_line()
g2 = g1 + coord_cartesian(xlim=c(0,0.25),ylim=c(0.75,1.0)) +
  theme(legend.position=c(0.75, 0.25))
g2 
```


```{r}
logreg_fit<-logreg_wf %>% fit(binary_training_data)
LDA_fit<-lda_wf %>% fit(binary_training_data)
QDA_fit<-qda_wf %>% fit(binary_training_data)
```

For each model, determine the threshold that maximizes the J-index using the training set. Why is the J-index a better metric than accuracy in this case? Create plots that show the dependence of the J-index from the threshold. 

```{r}
#| fig.cap: Table 2
#| out.width: 75%
performance_logreg<-logreg_fit %>% 
                    augment(binary_training_data) %>% 
                    probably::threshold_perf(Class, .pred_BlueTarp, 
                    thresholds=seq(0.01, 0.5, 0.001), event_level="first",
                    metrics=metric_set(j_index))
logreg_max_j_index <- performance_logreg %>% 
    #filter(.metric == "j_index") %>% 
    filter(.estimate == max(.estimate))

performance_LDA<-LDA_fit %>% 
                    augment(binary_training_data) %>% 
                    probably::threshold_perf(Class, .pred_BlueTarp, 
                    thresholds=seq(0.01, 0.5, 0.001), event_level="first",
                    metrics=metric_set(j_index))
LDA_max_j_index <- performance_LDA %>% 
    #filter(.metric == "j_index") %>% 
    filter(.estimate == max(.estimate))


performance_QDA<-QDA_fit  %>% 
                    augment(binary_training_data) %>% 
                    probably::threshold_perf(Class, .pred_BlueTarp, 
                    thresholds=seq(0.01, 0.5, 0.001), event_level="first",
                    metrics=metric_set(j_index))
QDA_max_j_index <- performance_QDA %>% 
    #filter(.metric == "j_index") %>% 
    filter(.estimate == max(.estimate))

Names<-c("Logistic Regression","LDA","QDA")

metrics_table <- function(metrics, caption) {
  metrics %>%
      pivot_wider(names_from=.metric, values_from=.estimate) %>%
      select(-.estimator) %>%
      knitr::kable(caption=caption, digits=5) %>%
      kableExtra::kable_styling(full_width=FALSE)
}

#Save Threshold values
logreg_thresh<-logreg_max_j_index$.threshold
LDA_thresh<-LDA_max_j_index$.threshold
QDA_thresh<-QDA_max_j_index$.threshold

metrics_table(bind_cols(Names, bind_rows(logreg_max_j_index,LDA_max_j_index,QDA_max_j_index)),
              "Thresholds")
```

Determine the accuracy, sensitivity, specificity, and J-index for each model at the determined thresholds. Which model performs best? How does this compare to the result from the ROC curves?
```{r augmenting dataset for each model}
augment_model<-function(model,data,thresh_level){
  model %>% 
    augment(data) %>% 
    mutate(pred=as.factor(ifelse(.pred_BlueTarp>= thresh_level,"BlueTarp","NotBlueTarp")))
}
final_logreg<-augment_model(logreg_fit,binary_training_data,logreg_thresh)
final_LDA<-augment_model(LDA_fit,binary_training_data,LDA_thresh)
final_QDA<-augment_model(QDA_fit,binary_training_data,QDA_thresh)


holdout_logreg<-augment_model(logreg_fit,holdout,logreg_thresh)
holdout_LDA<-augment_model(LDA_fit,holdout,LDA_thresh)
holdout_QDA<-augment_model(QDA_fit,holdout,QDA_thresh)
```   


```{r}
#This outputs a function
class_metrics<-metric_set(accuracy,sensitivity,specificity,j_index)
calculate_metrics <- function(model, train, test, model_name,thresh_level) {
    roc_auc(model %>% augment(train), Class, .pred_BlueTarp, event_level="first")
    bind_rows(
        bind_cols(
            model=model_name,
            dataset="train",
            class_metrics(model %>% augment_model(train,thresh_level), 
                          truth=Class, 
                          estimate=pred),
        ),
        bind_cols(
            model=model_name,
            dataset="train",
            roc_auc(model %>% augment_model(train,thresh_level),
                    Class,
                    .pred_BlueTarp,
                    event_level="first"),
        )
    )
}
```

```{r}
#accuracy, sensitivity, specificity, and J-index
ASSJ<-bind_rows(calculate_metrics(logreg_fit,binary_training_data,holdout,"Logistic",logreg_thresh),
          calculate_metrics(LDA_fit,binary_training_data,holdout,"LDA",LDA_thresh),
          calculate_metrics(QDA_fit,binary_training_data,holdout,"QDA",QDA_thresh))
metrics_table(ASSJ,"Threshold Metrics")
```

Similarly to the ROC Curves prior to determining the threshold LDA overall performs the worst on the training data. Logistic and QDA are once again more difficult to determine with some metrics pointing toward one model and others pointing to the other model. We know our dataset is very imbalanced and so the results of accuracy should not be taken into account. We are more concerned with the other remaining metrics. QDA has a higher sensitivity (True Positive Rate), which indicates that it is more likely to correctly classify the blue tarps when they truly are blue tarps. This means in comparison that Logisitic would be more likely to miss some blue tarps than QDA. Logistic has a higher specificity (True Negative Rate), which indicates that it is more likely to correctly classify the not blue tarps when they are truly not blue tarps. This means that in comparison that QDA would be more likely to miss some not blue tarps than Logisitic. The ROC AUC is relatively the same from cross validation and from threshold selection. 

```{r comparison-graph}
#| fig.cap: Distribution of the predicted probability for the BlueTarp and NotBlueTarp classes for the three classification models 
#| fig.width: 16
#| fig.height: 4
#| out.width: 100%
distribution_graph <- function(model, data, model_name) {
    model %>% 
        augment(data) %>%
    ggplot(aes(x=.pred_BlueTarp,color=.pred_class)) +
        geom_density(bw=0.07) +
        labs(x='Probability of Blue Tarp', title=model_name) + 
        scale_color_manual(breaks=c("BlueTarp","NotBlueTarp"), values=c("blue","red"))+
        theme(legend.position=c(0.5, 0.5))#"right")
}
g1 <- distribution_graph(logreg_fit, holdout, "Logistic regression")
g2 <- distribution_graph(LDA_fit, holdout, "LDA")
g3 <- distribution_graph(QDA_fit, holdout, "QDA")
g1 + g2 + g3
```

LDA and QDA look relatively similar. Logistic, however looks different, this will lead to a different optimal threshold value.

Proportion of Blue Tarp Identified in Holdout
```{r}
logreg_prop<-nrow(holdout_logreg[which(holdout_logreg$pred == "BlueTarp"),]) / nrow(holdout_logreg)
LDA_prop<-nrow(holdout_LDA[which(holdout_LDA$pred == "BlueTarp"),]) / nrow(holdout_LDA)
QDA_prop<-nrow(holdout_QDA[which(holdout_QDA$pred == "BlueTarp"),]) / nrow(holdout_QDA)
logreg_prop
LDA_prop
QDA_prop
```





Stop cluster
```{r}
stopCluster(cl)
registerDoSEQ()
```
